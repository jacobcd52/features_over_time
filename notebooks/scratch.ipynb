{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from buffer import MultiModelActivationBuffer\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from buffer import MultiModelActivationBuffer\n",
    "from trainers.top_k import TopKTrainer, AutoEncoderTopK\n",
    "from training import trainSAE\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 4\n",
    "expansion = 8\n",
    "num_tokens = int(1e6)\n",
    "out_batch_size = 4\n",
    "\n",
    "submodule_list = []\n",
    "model_list = []\n",
    "for step in [1, 16, 64, 256, 1000, 4000, 16000, 64000, 143000]:\n",
    "    model = LanguageModel(\"EleutherAI/pythia-70m\", revision=f\"step{step}\", trust_remote_code=False, device_map=device)\n",
    "    model_list.append(model)\n",
    "    submodule_list.append(model.gpt_neox.layers[layer])\n",
    "                       \n",
    "\n",
    "\n",
    "activation_dim = 512\n",
    "dictionary_size = expansion * activation_dim\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True,\n",
    "                                trust_remote_code=True)\n",
    "\n",
    "class CustomData():\n",
    "    def __init__(self, dataset):\n",
    "        self.data = iter(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.data)['text']\n",
    "\n",
    "data = CustomData(dataset)\n",
    "\n",
    "\n",
    "buffer = MultiModelActivationBuffer(\n",
    "    data=data,\n",
    "    model_list=model_list,\n",
    "    submodule_list=submodule_list,\n",
    "    d_submodule=activation_dim, # output dimension of the model component\n",
    "    n_ctxs=128,  # you can set this higher or lower dependong on your available memory\n",
    "    device=device,\n",
    "    refresh_batch_size=32,\n",
    "    out_batch_size=out_batch_size,\n",
    ")  # buffer will yield batches of tensors of dimension = submodule's output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4608])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(buffer).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 680/250000 [00:16<1:41:07, 41.09it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m: TopKTrainer,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: AutoEncoderTopK,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# train the sparse autoencoder (SAE)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m ae \u001b[38;5;241m=\u001b[39m \u001b[43mtrainSAE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# you could also use another (i.e. pytorch dataloader) here instead of buffer\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtrainer_cfg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocast_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/features_over_time/notebooks/../training.py:238\u001b[0m, in \u001b[0;36mtrainSAE\u001b[0;34m(data, trainer_configs, steps, use_wandb, wandb_entity, wandb_project, save_steps, save_dir, log_steps, activations_split_by_head, transcoder, run_cfg, normalize_activations, verbose, device, autocast_dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trainer \u001b[38;5;129;01min\u001b[39;00m trainers:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m autocast_context:\n\u001b[0;32m--> 238\u001b[0m             \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# save final SAEs\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m save_dir, trainer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(save_dirs, trainers):\n",
      "File \u001b[0;32m~/features_over_time/notebooks/../trainers/top_k.py:343\u001b[0m, in \u001b[0;36mTopKTrainer.update\u001b[0;34m(self, step, x)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Make sure the decoder is still unit-norm\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mae\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m set_decoder_norm_to_unit_norm(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mae\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mae\u001b[38;5;241m.\u001b[39mactivation_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mae\u001b[38;5;241m.\u001b[39mdict_size\n\u001b[1;32m    341\u001b[0m )\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_cfg = {\n",
    "    \"trainer\": TopKTrainer,\n",
    "    \"dict_class\": AutoEncoderTopK,\n",
    "    \"activation_dim\": activation_dim * len(model_list),\n",
    "    \"dict_size\": dictionary_size,\n",
    "    \"lr\": 1e-3,\n",
    "    \"device\": device,\n",
    "    \"steps\": num_tokens // out_batch_size,\n",
    "    \"k\": 128,\n",
    "    \"layer\": layer,\n",
    "    \"lm_name\": \"blah\",\n",
    "    \"warmup_steps\": 0,\n",
    "}\n",
    "\n",
    "# train the sparse autoencoder (SAE)\n",
    "ae = trainSAE(\n",
    "    data=buffer,  # you could also use another (i.e. pytorch dataloader) here instead of buffer\n",
    "    trainer_configs=[trainer_cfg],\n",
    "    steps=num_tokens // out_batch_size,\n",
    "    autocast_dtype=t.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
